# -*- coding: utf-8 -*-
"""net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H4RZVIeNbCalxzLVG5-3HLTdIDK06uu8
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
from random import randint
import os
from warnings import filterwarnings
filterwarnings('ignore')

from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model, model_from_json

# %matplotlib inline

train_samples = []
train_labels = []

# generating a synthetic train data

for i in range(50):
  # The ~5% of younger individuals who did experience side effect
  random_younger = randint(13, 64)
  train_samples.append(random_younger)
  train_labels.append(1)

  # The ~5% of older individuals who did not experience side effects
  random_older = randint(64, 100)
  train_samples.append(random_older)
  train_labels.append(0)

for i in range(1000):
  # The ~95% of younger individuals who did not experience side effect
  random_younger = randint(13, 64)
  train_samples.append(random_younger)
  train_labels.append(0)

  # The ~95% of older individuals who did not experience side effects
  random_older = randint(64, 100)
  train_samples.append(random_older)
  train_labels.append(1)

train_samples[:10], train_labels[:10]

train_samples = np.array(train_samples)
train_labels = np.array(train_labels)
train_labels, train_samples = shuffle(train_labels, train_samples)

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_trained_samples = scaler.fit_transform(train_samples.reshape(-1, 1))

scaled_trained_samples[:10]

physical_device = tf.config.experimental.list_physical_devices('TPU')
print('Number of GPUs available: ', len(physical_device))
# tf.config.experimental.set_memory_growth(physical_device[0], True)

model = Sequential([
    Dense(units=16, input_shape=(1,), activation='relu'),
    Dense(units=32, activation='relu'),
    Dense(units=2, activation='softmax')
])

model.summary()

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    x=scaled_trained_samples,
    y=train_labels,
    validation_split=0.1,
    batch_size=10,
    epochs=30,
    shuffle=True,
    verbose=2
)

history.history.keys()

# generating a synthetic test data

test_samples = []
test_labels = []

for i in range(10):
  # The ~5% of younger individuals who did experience side effect
  random_younger = randint(13, 64)
  test_samples.append(random_younger)
  test_labels.append(1)

  # The ~5% of older individuals who did not experience side effects
  random_older = randint(64, 100)
  test_samples.append(random_older)
  test_labels.append(0)

for i in range(200):
  # The ~95% of younger individuals who did not experience side effect
  random_younger = randint(13, 64)
  test_samples.append(random_younger)
  test_labels.append(0)

  # The ~95% of older individuals who did not experience side effects
  random_older = randint(64, 100)
  test_samples.append(random_older)
  test_labels.append(1)

test_labels = np.array(test_labels)
test_samples = np.array(test_samples)
test_samples, test_labels = shuffle(test_samples, test_labels)

scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1, 1))

predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=2)

predictions[:10]

rounded_predictions = np.argmax(predictions, axis=-1)
rounded_predictions[:10]

cm = confusion_matrix(test_labels, rounded_predictions)

cm

disp = ConfusionMatrixDisplay(cm)
disp.plot()
plt.show()

if os.path.isfile('models/classnet.h5') is False:
  model.save('model/classnet.h5')

new_model = load_model('model/classnet.h5')

new_model.summary()

new_model.get_weights()

new_model.optimizer

# model to JSON

json_string = model.to_json()
json_string

# save to YAML

# yaml_string = model.to_yaml()
# yaml_string

# model construction from JSON
model_architecture = model_from_json(json_string)

model_architecture.summary()